{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline으로 전처리 - 모델링 - 예측까지 한번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikitlearn update\n",
    "# %conda install -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_scr_pv</th>\n",
       "      <th>c_temp_pv</th>\n",
       "      <th>k_rpm_pv</th>\n",
       "      <th>n_temp_pv</th>\n",
       "      <th>scale_pv</th>\n",
       "      <th>s_temp_pv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.9</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143078</th>\n",
       "      <td>8</td>\n",
       "      <td>70.3</td>\n",
       "      <td>185</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143081</th>\n",
       "      <td>8</td>\n",
       "      <td>70.4</td>\n",
       "      <td>185</td>\n",
       "      <td>66.8</td>\n",
       "      <td>3.03</td>\n",
       "      <td>67.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143082</th>\n",
       "      <td>8</td>\n",
       "      <td>70.4</td>\n",
       "      <td>185</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.03</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143086</th>\n",
       "      <td>8</td>\n",
       "      <td>70.4</td>\n",
       "      <td>185</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.06</td>\n",
       "      <td>66.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143090</th>\n",
       "      <td>8</td>\n",
       "      <td>70.3</td>\n",
       "      <td>185</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.05</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28097 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        E_scr_pv  c_temp_pv  k_rpm_pv  n_temp_pv  scale_pv  s_temp_pv\n",
       "0              8       69.6       189       67.2      3.01       67.1\n",
       "1              8       69.8       189       67.2      3.01       67.0\n",
       "2              8       69.7       189       67.9      3.08       65.9\n",
       "3              8       69.7       189       67.8      3.08       65.9\n",
       "4              8       69.7       189       67.8      3.08       65.9\n",
       "...          ...        ...       ...        ...       ...        ...\n",
       "143078         8       70.3       185       66.7      3.01       67.7\n",
       "143081         8       70.4       185       66.8      3.03       67.3\n",
       "143082         8       70.4       185       66.7      3.03       67.2\n",
       "143086         8       70.4       185       66.7      3.06       66.9\n",
       "143090         8       70.3       185       66.7      3.05       66.5\n",
       "\n",
       "[28097 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../DATA/바웰공정데이터.csv')\n",
    "\n",
    "# 2. Preprocessing : 목요일까지의 전처리\n",
    "# (1) 2 < scale_pv < 4\n",
    "data = data[(data['scale_pv'] > 2) & (data['scale_pv'] < 4)]\n",
    "\n",
    "# (2) k_rpm_pv 가 100 이하인 행 제거\n",
    "data = data[data['k_rpm_pv'] > 100]\n",
    "\n",
    "# (3) n_temp_sv=0 인 행 제거\n",
    "data = data[data['n_temp_sv'] != 0]\n",
    "\n",
    "# (4) 컬럼 제거 : E_scr_sv, c_temp_sv, n_temp_sv, s_temp_sv, k_rpm_sv, time\n",
    "data = data.drop(['E_scr_sv', 'c_temp_sv', 'n_temp_sv', 's_temp_sv', \"k_rpm_sv\", 'time'], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330.1299999999978\n",
      "1179.1899999999987\n"
     ]
    }
   ],
   "source": [
    "# scale의 3과의 오차 계산\n",
    "# 1. 절대값\n",
    "data['scale_error_abs'] = abs(data['scale_pv'] - 3)\n",
    "\n",
    "# 2. 순수 오차\n",
    "data['scale_error'] = data['scale_pv'] - 3\n",
    "\n",
    "# 각 합계 계산\n",
    "print(data['scale_error_abs'].sum())\n",
    "print(data['scale_error'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_scr_pv</th>\n",
       "      <th>c_temp_pv</th>\n",
       "      <th>k_rpm_pv</th>\n",
       "      <th>n_temp_pv</th>\n",
       "      <th>scale_pv</th>\n",
       "      <th>s_temp_pv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.4</td>\n",
       "      <td>3.01</td>\n",
       "      <td>65.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>189</td>\n",
       "      <td>67.4</td>\n",
       "      <td>3.01</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>189</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.02</td>\n",
       "      <td>68.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143073</th>\n",
       "      <td>8</td>\n",
       "      <td>70.4</td>\n",
       "      <td>185</td>\n",
       "      <td>66.9</td>\n",
       "      <td>3.03</td>\n",
       "      <td>68.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143077</th>\n",
       "      <td>8</td>\n",
       "      <td>70.3</td>\n",
       "      <td>185</td>\n",
       "      <td>66.8</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143078</th>\n",
       "      <td>8</td>\n",
       "      <td>70.3</td>\n",
       "      <td>185</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143081</th>\n",
       "      <td>8</td>\n",
       "      <td>70.4</td>\n",
       "      <td>185</td>\n",
       "      <td>66.8</td>\n",
       "      <td>3.03</td>\n",
       "      <td>67.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143082</th>\n",
       "      <td>8</td>\n",
       "      <td>70.4</td>\n",
       "      <td>185</td>\n",
       "      <td>66.7</td>\n",
       "      <td>3.03</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14859 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        E_scr_pv  c_temp_pv  k_rpm_pv  n_temp_pv  scale_pv  s_temp_pv\n",
       "0              8       69.6       189       67.2      3.01       67.1\n",
       "1              8       69.8       189       67.2      3.01       67.0\n",
       "5              8       69.7       189       67.4      3.01       65.8\n",
       "6              8       69.8       189       67.4      3.01       66.0\n",
       "7              8       69.8       189       66.7      3.02       68.1\n",
       "...          ...        ...       ...        ...       ...        ...\n",
       "143073         8       70.4       185       66.9      3.03       68.3\n",
       "143077         8       70.3       185       66.8      3.01       67.9\n",
       "143078         8       70.3       185       66.7      3.01       67.7\n",
       "143081         8       70.4       185       66.8      3.03       67.3\n",
       "143082         8       70.4       185       66.7      3.03       67.2\n",
       "\n",
       "[14859 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-2. Preprocessing : 추가 전처리\n",
    "\n",
    "# 2.85 ~ 3.3 사이의 scale 개수\n",
    "data[(data['scale_pv'] > 2.95) & (data['scale_pv'] < 3.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scale boxplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot(data['scale_pv'], vert=False)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(data['scale_pv'], bins=100, color='skyblue', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1254.6599999999983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjs31\\AppData\\Local\\Temp\\ipykernel_4428\\3223128013.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_over_3['scale_error'] = data_over_3['scale_pv'] - 3\n"
     ]
    }
   ],
   "source": [
    "# 3이상의 scale 의 3과의 오차 계산\n",
    "# 4 > scale > 3\n",
    "data_over_3 = data[data['scale_pv'] > 3]\n",
    "data_over_3['scale_error'] = data_over_3['scale_pv'] - 3\n",
    "print(data_over_3['scale_error'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2519\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 중복값 확인\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# 중복값 제거\n",
    "data = data.drop_duplicates()\n",
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 209\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "NoScaler_LinearRegression - MAE : 0.0282\n",
      "NoScaler_LinearRegression - MAPE : 0.009285\n",
      "NoScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "NoScaler_ElasticNet - MAE : 0.0288\n",
      "NoScaler_ElasticNet - MAPE : 0.009469\n",
      "NoScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "NoScaler_RandomForestRegressor - MAE : 0.0225\n",
      "NoScaler_RandomForestRegressor - MAPE : 0.007422\n",
      "NoScaler_RandomForestRegressor - R2 : 0.4214\n",
      "----------------------------------\n",
      "NoScaler_LGBMRegressor - MAE : 0.0256\n",
      "NoScaler_LGBMRegressor - MAPE : 0.008420\n",
      "NoScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "MinMaxScaler_LinearRegression - MAE : 0.0282\n",
      "MinMaxScaler_LinearRegression - MAPE : 0.009285\n",
      "MinMaxScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "StandardScaler_LinearRegression - MAE : 0.0282\n",
      "StandardScaler_LinearRegression - MAPE : 0.009285\n",
      "StandardScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "RobustScaler_LinearRegression - MAE : 0.0282\n",
      "RobustScaler_LinearRegression - MAPE : 0.009285\n",
      "RobustScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "MinMaxScaler_ElasticNet - MAE : 0.0288\n",
      "MinMaxScaler_ElasticNet - MAPE : 0.009469\n",
      "MinMaxScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "StandardScaler_ElasticNet - MAE : 0.0288\n",
      "StandardScaler_ElasticNet - MAPE : 0.009469\n",
      "StandardScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "RobustScaler_ElasticNet - MAE : 0.0288\n",
      "RobustScaler_ElasticNet - MAPE : 0.009469\n",
      "RobustScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "MinMaxScaler_RandomForestRegressor - MAE : 0.0226\n",
      "MinMaxScaler_RandomForestRegressor - MAPE : 0.007447\n",
      "MinMaxScaler_RandomForestRegressor - R2 : 0.4150\n",
      "----------------------------------\n",
      "StandardScaler_RandomForestRegressor - MAE : 0.0225\n",
      "StandardScaler_RandomForestRegressor - MAPE : 0.007395\n",
      "StandardScaler_RandomForestRegressor - R2 : 0.4244\n",
      "----------------------------------\n",
      "RobustScaler_RandomForestRegressor - MAE : 0.0226\n",
      "RobustScaler_RandomForestRegressor - MAPE : 0.007454\n",
      "RobustScaler_RandomForestRegressor - R2 : 0.4166\n",
      "----------------------------------\n",
      "MinMaxScaler_LGBMRegressor - MAE : 0.0256\n",
      "MinMaxScaler_LGBMRegressor - MAPE : 0.008420\n",
      "MinMaxScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "StandardScaler_LGBMRegressor - MAE : 0.0256\n",
      "StandardScaler_LGBMRegressor - MAPE : 0.008420\n",
      "StandardScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "RobustScaler_LGBMRegressor - MAE : 0.0256\n",
      "RobustScaler_LGBMRegressor - MAPE : 0.008420\n",
      "RobustScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature Engineering\n",
    "# - Pileline으로 스케일링 및 모델링을 한번에 처리\n",
    "# - Scaling : MinMaxScaler, StandardScaler, RobustScaler, 스케일링 없이 하나\n",
    "# - Model : LinearRegression, ElasticNet, RandomForest, LightGBM\n",
    "# - Evaluation : MAE, MAPE, R2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 3-1. 데이터 분할\n",
    "X = data.drop('scale_pv', axis=1)\n",
    "y = data['scale_pv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create pipelines for each scaling method and model\n",
    "pipelines = {    \n",
    "    'NoScaler_LinearRegression': Pipeline([('model', LinearRegression())]),\n",
    "    'NoScaler_ElasticNet': Pipeline([('model', ElasticNet())]),\n",
    "    'NoScaler_RandomForestRegressor': Pipeline([('model', RandomForestRegressor())]),\n",
    "    'NoScaler_LGBMRegressor': Pipeline([('model', LGBMRegressor())]),\n",
    "    'MinMaxScaler_LinearRegression': Pipeline([('scaler', MinMaxScaler()), ('model', LinearRegression())]),\n",
    "    'StandardScaler_LinearRegression': Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),\n",
    "    'RobustScaler_LinearRegression': Pipeline([('scaler', RobustScaler()), ('model', LinearRegression())]),\n",
    "    'MinMaxScaler_ElasticNet': Pipeline([('scaler', MinMaxScaler()), ('model', ElasticNet())]),\n",
    "    'StandardScaler_ElasticNet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNet())]),\n",
    "    'RobustScaler_ElasticNet': Pipeline([('scaler', RobustScaler()), ('model', ElasticNet())]),\n",
    "    'MinMaxScaler_RandomForestRegressor': Pipeline([('scaler', MinMaxScaler()), ('model', RandomForestRegressor())]),\n",
    "    'StandardScaler_RandomForestRegressor': Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())]),\n",
    "    'RobustScaler_RandomForestRegressor': Pipeline([('scaler', RobustScaler()), ('model', RandomForestRegressor())]),\n",
    "    'MinMaxScaler_LGBMRegressor': Pipeline([('scaler', MinMaxScaler()), ('model', LGBMRegressor())]),\n",
    "    'StandardScaler_LGBMRegressor': Pipeline([('scaler', StandardScaler()), ('model', LGBMRegressor())]),\n",
    "    'RobustScaler_LGBMRegressor': Pipeline([('scaler', RobustScaler()), ('model', LGBMRegressor())]),\n",
    "}\n",
    "\n",
    "# Fit the pipeline\n",
    "for pipeline in pipelines.values():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate the pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f'{name} - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "    print(f'{name} - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "    print(f'{name} - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    # MAPE가 가장 낮은 모델을 선택\n",
    "    if name == 'NoScaler_LinearRegression':\n",
    "        best_model = pipeline\n",
    "        best_scaler = 'NoScaler'\n",
    "        best_model_name = 'LinearRegression'\n",
    "        best_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    else:\n",
    "        if mean_absolute_percentage_error(y_test, y_pred) < best_mape:\n",
    "            best_model = pipeline\n",
    "            best_scaler = name.split('_')[0]\n",
    "            best_model_name = name.split('_')[1]\n",
    "            best_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model : StandardScaler_RandomForestRegressor\n",
      "Best MAPE : 0.007395\n",
      "Best R2 : 0.4244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>3.07</td>\n",
       "      <td>3.054550</td>\n",
       "      <td>0.015450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52197</th>\n",
       "      <td>3.02</td>\n",
       "      <td>3.027400</td>\n",
       "      <td>-0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84861</th>\n",
       "      <td>3.01</td>\n",
       "      <td>3.061150</td>\n",
       "      <td>-0.051150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44733</th>\n",
       "      <td>3.08</td>\n",
       "      <td>3.034900</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>3.03</td>\n",
       "      <td>3.029900</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128485</th>\n",
       "      <td>3.04</td>\n",
       "      <td>3.032200</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135932</th>\n",
       "      <td>3.06</td>\n",
       "      <td>3.054900</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>3.04</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70281</th>\n",
       "      <td>3.07</td>\n",
       "      <td>3.061900</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74593</th>\n",
       "      <td>3.01</td>\n",
       "      <td>3.060544</td>\n",
       "      <td>-0.050544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5116 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_true    y_pred     error\n",
       "2988      3.07  3.054550  0.015450\n",
       "52197     3.02  3.027400 -0.007400\n",
       "84861     3.01  3.061150 -0.051150\n",
       "44733     3.08  3.034900  0.045100\n",
       "5194      3.03  3.029900  0.000100\n",
       "...        ...       ...       ...\n",
       "128485    3.04  3.032200  0.007800\n",
       "135932    3.06  3.054900  0.005100\n",
       "7936      3.04  3.040300 -0.000300\n",
       "70281     3.07  3.061900  0.008100\n",
       "74593     3.01  3.060544 -0.050544\n",
       "\n",
       "[5116 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best model\n",
    "print(f'Best Model : {best_scaler}_{best_model_name}')\n",
    "print(f'Best MAPE : {best_mape:.6f}')\n",
    "print(f'Best R2 : {r2_score(y_test, best_model.predict(X_test)):.4f}')\n",
    "\n",
    "# 예측값과 실제값, 오차를 DataFrame으로 저장\n",
    "result = pd.DataFrame({'y_true': y_test, 'y_pred': best_model.predict(X_test), 'error': y_test - best_model.predict(X_test)})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>3.07</td>\n",
       "      <td>3.054550</td>\n",
       "      <td>0.015450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52197</th>\n",
       "      <td>3.02</td>\n",
       "      <td>3.027400</td>\n",
       "      <td>-0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84861</th>\n",
       "      <td>3.01</td>\n",
       "      <td>3.061150</td>\n",
       "      <td>-0.051150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44733</th>\n",
       "      <td>3.08</td>\n",
       "      <td>3.034900</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>3.03</td>\n",
       "      <td>3.029900</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128485</th>\n",
       "      <td>3.04</td>\n",
       "      <td>3.032200</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135932</th>\n",
       "      <td>3.06</td>\n",
       "      <td>3.054900</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>3.04</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70281</th>\n",
       "      <td>3.07</td>\n",
       "      <td>3.061900</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74593</th>\n",
       "      <td>3.01</td>\n",
       "      <td>3.060544</td>\n",
       "      <td>-0.050544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4785 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_true    y_pred     error\n",
       "2988      3.07  3.054550  0.015450\n",
       "52197     3.02  3.027400 -0.007400\n",
       "84861     3.01  3.061150 -0.051150\n",
       "44733     3.08  3.034900  0.045100\n",
       "5194      3.03  3.029900  0.000100\n",
       "...        ...       ...       ...\n",
       "128485    3.04  3.032200  0.007800\n",
       "135932    3.06  3.054900  0.005100\n",
       "7936      3.04  3.040300 -0.000300\n",
       "70281     3.07  3.061900  0.008100\n",
       "74593     3.01  3.060544 -0.050544\n",
       "\n",
       "[4785 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_true 2.9 ~ 3.1 사이의 값만 추출\n",
    "result[(result['y_true'] > 2.9) & (result['y_true'] < 3.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wjs31\\OneDrive\\문서\\KDT5\\KDT5_NLP_Project\\.conda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.8349 - mape: 27.4408 - val_loss: 0.0352 - val_mape: 1.1546\n",
      "Epoch 2/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0347 - mape: 1.1414 - val_loss: 0.0353 - val_mape: 1.1553\n",
      "Epoch 3/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0320 - mape: 1.0525 - val_loss: 0.0326 - val_mape: 1.0789\n",
      "Epoch 4/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0327 - mape: 1.0766 - val_loss: 0.0302 - val_mape: 0.9942\n",
      "Epoch 5/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0312 - mape: 1.0255 - val_loss: 0.0283 - val_mape: 0.9315\n",
      "Epoch 6/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0317 - mape: 1.0444 - val_loss: 0.0278 - val_mape: 0.9180\n",
      "Epoch 7/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0309 - mape: 1.0182 - val_loss: 0.0289 - val_mape: 0.9546\n",
      "Epoch 8/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0315 - mape: 1.0374 - val_loss: 0.0387 - val_mape: 1.2821\n",
      "Epoch 9/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0323 - mape: 1.0651 - val_loss: 0.0289 - val_mape: 0.9566\n",
      "Epoch 10/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0307 - mape: 1.0112 - val_loss: 0.0298 - val_mape: 0.9864\n",
      "Epoch 11/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0305 - mape: 1.0041 - val_loss: 0.0277 - val_mape: 0.9121\n",
      "Epoch 12/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0303 - mape: 0.9972 - val_loss: 0.0311 - val_mape: 1.0297\n",
      "Epoch 13/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0306 - mape: 1.0093 - val_loss: 0.0352 - val_mape: 1.1539\n",
      "Epoch 14/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0292 - mape: 0.9618 - val_loss: 0.0322 - val_mape: 1.0560\n",
      "Epoch 15/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0301 - mape: 0.9905 - val_loss: 0.0291 - val_mape: 0.9622\n",
      "Epoch 16/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0302 - mape: 0.9947 - val_loss: 0.0279 - val_mape: 0.9194\n",
      "Epoch 17/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0300 - mape: 0.9892 - val_loss: 0.0279 - val_mape: 0.9184\n",
      "Epoch 18/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0293 - mape: 0.9636 - val_loss: 0.0306 - val_mape: 1.0049\n",
      "Epoch 19/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0290 - mape: 0.9558 - val_loss: 0.0306 - val_mape: 1.0129\n",
      "Epoch 20/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0293 - mape: 0.9661 - val_loss: 0.0278 - val_mape: 0.9170\n",
      "Epoch 21/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0291 - mape: 0.9589 - val_loss: 0.0278 - val_mape: 0.9196\n",
      "Epoch 22/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0295 - mape: 0.9725 - val_loss: 0.0306 - val_mape: 1.0051\n",
      "Epoch 23/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0284 - mape: 0.9359 - val_loss: 0.0278 - val_mape: 0.9167\n",
      "Epoch 24/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0296 - mape: 0.9758 - val_loss: 0.0277 - val_mape: 0.9117\n",
      "Epoch 25/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0290 - mape: 0.9559 - val_loss: 0.0281 - val_mape: 0.9269\n",
      "Epoch 26/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0283 - mape: 0.9331 - val_loss: 0.0347 - val_mape: 1.1508\n",
      "Epoch 27/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0306 - mape: 1.0071 - val_loss: 0.0302 - val_mape: 1.0007\n",
      "Epoch 28/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0299 - mape: 0.9854 - val_loss: 0.0279 - val_mape: 0.9188\n",
      "Epoch 29/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0291 - mape: 0.9566 - val_loss: 0.0276 - val_mape: 0.9082\n",
      "Epoch 30/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0285 - mape: 0.9390 - val_loss: 0.0281 - val_mape: 0.9263\n",
      "Epoch 31/1000\n",
      "\u001b[1m467/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0286 - mape: 0.9417"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m X_test_scaled\u001b[38;5;241m.\u001b[39mreshape(X_test_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_test_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m     35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[1;32mc:\\Users\\wjs31\\OneDrive\\문서\\KDT5\\KDT5_NLP_Project\\.conda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\wjs31\\OneDrive\\문서\\KDT5\\KDT5_NLP_Project\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:316\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    314\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m--> 316\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wjs31\\OneDrive\\문서\\KDT5\\KDT5_NLP_Project\\.conda\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:103\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    101\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_train_batch_begin(batch, logs\u001b[38;5;241m=\u001b[39mlogs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    104\u001b[0m     logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM 모델링\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터 전처리\n",
    "X = data.drop('scale_pv', axis=1)\n",
    "y = data['scale_pv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# LSTM 모델링\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train_scaled.shape[1], 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mape'])\n",
    "\n",
    "# EarlyStopping\n",
    "es = EarlyStopping(monitor='mape', mode='min', verbose=1, patience=30)\n",
    "\n",
    "# Reshape\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "# Evaluate\n",
    "print(f'LSTM - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'LSTM - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'LSTM - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "512/512 [==============================] - 6s 6ms/step - loss: 0.3779 - val_loss: 0.0075\n",
      "Epoch 2/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0408 - val_loss: 0.0020\n",
      "Epoch 3/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0375 - val_loss: 0.0043\n",
      "Epoch 4/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0373 - val_loss: 0.0033\n",
      "Epoch 5/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0366 - val_loss: 0.0033\n",
      "Epoch 6/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0358 - val_loss: 0.0020\n",
      "Epoch 7/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0338 - val_loss: 0.0034\n",
      "Epoch 8/1000\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.0322 - val_loss: 0.0018\n",
      "Epoch 9/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0301 - val_loss: 0.0030\n",
      "Epoch 10/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0288 - val_loss: 0.0018\n",
      "Epoch 11/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0264 - val_loss: 0.0026\n",
      "Epoch 12/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0244 - val_loss: 0.0019\n",
      "Epoch 13/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0223 - val_loss: 0.0018\n",
      "Epoch 14/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0203 - val_loss: 0.0021\n",
      "Epoch 15/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0187 - val_loss: 0.0019\n",
      "Epoch 16/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0170 - val_loss: 0.0019\n",
      "Epoch 17/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0156 - val_loss: 0.0021\n",
      "Epoch 18/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0138 - val_loss: 0.0017\n",
      "Epoch 19/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0118 - val_loss: 0.0017\n",
      "Epoch 20/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0109 - val_loss: 0.0020\n",
      "Epoch 21/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0095 - val_loss: 0.0017\n",
      "Epoch 22/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0083 - val_loss: 0.0019\n",
      "Epoch 23/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0073 - val_loss: 0.0017\n",
      "Epoch 24/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0064 - val_loss: 0.0017\n",
      "Epoch 25/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0056 - val_loss: 0.0018\n",
      "Epoch 26/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0048 - val_loss: 0.0017\n",
      "Epoch 27/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 28/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 29/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 30/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 31/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 32/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 33/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 34/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 35/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 36/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 37/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 38/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 39/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 40/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 41/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 42/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 43/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 44/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 45/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 46/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 47/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 48/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 49/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 50/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 51/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 52/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 53/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 54/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 55/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 56/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 57/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 58/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 59/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 60/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 61/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 62/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 63/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 64/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 65/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 66/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 67/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 68/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 69/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 70/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 71/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 72/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 74/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 75/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 76/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 77/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 78/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 79/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 80/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 81/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 82/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 83/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 84/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 85/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 86/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 87/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 88/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 89/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 90/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 91/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 92/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 93/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 94/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 95/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 96/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 97/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 98/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 99/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 100/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 101/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 102/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 103/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 104/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 105/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 106/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 107/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 108/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 109/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 110/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 111/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 112/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 113/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 114/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 115/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 116/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 117/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 118/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 119/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 120/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 121/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 122/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 123/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 124/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 125/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 126/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 127/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 128/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 129/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 130/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 131/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 132/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 133/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 134/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 135/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 136/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 137/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 138/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 139/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 140/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 141/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 142/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 143/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 144/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 145/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 146/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 147/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 148/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 149/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 150/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 151/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 152/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 153/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 154/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 155/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 156/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 157/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 158/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 159/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 160/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 161/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 162/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 163/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 164/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 165/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 166/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 167/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 168/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 169/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 170/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 171/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 172/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 173/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 174/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 175/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 176/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 177/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 178/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 179/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 180/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 181/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 182/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 183/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 184/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 185/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 186/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 187/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 188/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 189/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 190/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 191/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 192/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 193/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 193: early stopping\n",
      "160/160 [==============================] - 1s 2ms/step\n",
      "LSTM - MAE : 0.0260\n",
      "LSTM - MAPE : 0.008549\n",
      "LSTM - R2 : 0.2572\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# LSTM 모델링\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train_scaled.shape[1], 1), return_sequences=True))   # return_sequences=True : 다음 LSTM 레이어에 전달\n",
    "model.add(Dropout(0.2))         # 과적합 방지\n",
    "model.add(LSTM(64))             # return_sequences=False : Dense 레이어에 전달\n",
    "model.add(Dropout(0.2))     \n",
    "model.add(Dense(1))            # 출력 레이어\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "# EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "# Evaluate\n",
    "print(f'LSTM - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'LSTM - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'LSTM - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "158 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "382 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.02321037 -0.02318184 -0.02314804\n",
      " -0.02323867 -0.02317189 -0.0231598  -0.0233736  -0.02335549 -0.0233262\n",
      " -0.02341289 -0.02339148 -0.02339357 -0.02340419 -0.02338292 -0.02336539\n",
      " -0.02354822 -0.02353781 -0.02352788 -0.02411196 -0.02410312 -0.02408227\n",
      " -0.02411808 -0.0241002  -0.02406409 -0.02411793 -0.02408705 -0.02408672\n",
      " -0.02322437 -0.0231537  -0.02315776 -0.02320221 -0.02316227 -0.02314438\n",
      " -0.02337348 -0.02332632 -0.02334039 -0.02341359 -0.02338564 -0.0233656\n",
      " -0.02341986 -0.0234015  -0.02337301 -0.02359435 -0.02355243 -0.02352629\n",
      " -0.02414277 -0.02408668 -0.02407452 -0.02412351 -0.02409236 -0.02406383\n",
      " -0.02411244 -0.02407776 -0.02406925         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.0255646  -0.02551335 -0.02553127 -0.02559765 -0.02557327 -0.02558189\n",
      " -0.02565836 -0.02565886 -0.02563291 -0.02562876 -0.02564037 -0.02563063\n",
      " -0.02565695 -0.02564607 -0.02562919 -0.02565231 -0.02568599 -0.02566505\n",
      " -0.0258015  -0.02578852 -0.02580692 -0.02583817 -0.02581067 -0.02580554\n",
      " -0.02584014 -0.02579529 -0.02581096 -0.02554682 -0.02554778 -0.02553887\n",
      " -0.02560042 -0.02557934 -0.02558498 -0.02570055 -0.02566116 -0.0256551\n",
      " -0.02565608 -0.02564634 -0.02563391 -0.02562724 -0.02563742 -0.02563089\n",
      " -0.02565926 -0.02568282 -0.02567171 -0.0258194  -0.02581969 -0.02581323\n",
      " -0.0257885  -0.02578278 -0.02580032 -0.02582732 -0.02580875 -0.02582195\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.02281518 -0.02278578 -0.02278329\n",
      " -0.02303945 -0.02301741 -0.02302806 -0.02338364 -0.0233049  -0.02332227\n",
      " -0.02344335 -0.02335387 -0.02332203 -0.02337006 -0.02335225 -0.02334947\n",
      " -0.02355288 -0.02353937 -0.02352391 -0.02415491 -0.02409559 -0.0241298\n",
      " -0.02415395 -0.02414161 -0.02409183 -0.02415436 -0.02412562 -0.02410973\n",
      " -0.02279124 -0.02275889 -0.02274371 -0.02310891 -0.02300374 -0.02300998\n",
      " -0.02337206 -0.0233327  -0.02332382 -0.02337749 -0.0233712  -0.02337344\n",
      " -0.02340707 -0.02337952 -0.02336343 -0.02358662 -0.02356063 -0.0235165\n",
      " -0.02415319 -0.0241226  -0.02414447 -0.0241348  -0.02413354 -0.02411914\n",
      " -0.02414652 -0.02414096 -0.02411607         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.02320318 -0.02315266 -0.02315502 -0.02316211 -0.02314126 -0.02314008\n",
      " -0.02340775 -0.02330858 -0.02332535 -0.02345911 -0.02337235 -0.02335126\n",
      " -0.02341086 -0.02339841 -0.02336137 -0.02355216 -0.02352829 -0.02349955\n",
      " -0.02413765 -0.02407968 -0.02409003 -0.02411304 -0.02409371 -0.02408027\n",
      " -0.02411562 -0.02411706 -0.02408672 -0.02317178 -0.02314461 -0.02312269\n",
      " -0.02319642 -0.02313015 -0.02313588 -0.02338376 -0.02332314 -0.02331529\n",
      " -0.0234082  -0.02339299 -0.02337586 -0.0234307  -0.02338273 -0.02337028\n",
      " -0.02356524 -0.02353902 -0.02351752 -0.02411129 -0.02409707 -0.02407715\n",
      " -0.02411361 -0.02410362 -0.02410518 -0.02415909 -0.02407022 -0.02408547]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__max_depth': 20, 'model__max_features': 'log2', 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 300}\n",
      "-0.02274371459495446\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 RandomForestRegressor(max_depth=20, max_features='log2',\n",
      "                                       n_estimators=300))])\n",
      "----------------------------------\n",
      "Best Model - MAE : 0.0223\n",
      "Best Model - MAPE : 0.007349\n",
      "Best Model - R2 : 0.4423\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4. Hyperparameter Tuning : GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())])\n",
    "\n",
    "# Create a parameter grid : RandomForestRegressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 4, 6],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=3)\n",
    "\n",
    "# Fit the GridSearchCV object\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Show the best hyperparameters\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)\n",
    "print('----------------------------------')\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = grid.predict(X_test)\n",
    "print(f'Best Model - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'Best Model - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'Best Model - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_MML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
